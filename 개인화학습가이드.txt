[Project MUSE] 나만의 AI 모델 만들기 가이드 (v11 Hybrid)

이 가이드는 muse_studio.py를 사용하여 데이터를 수집하고, 하이브리드 엔진에 적용할 개인화 모델을 학습하는 전체 과정을 설명합니다.

핵심 원리:

기본 상태: 학습된 모델이 없으면 내장된 **MODNet(배경)**과 **ViTPose(뼈대)**가 작동합니다.

학습 후: 특정 프로파일(예: front)에 대한 학습이 완료되면, 해당 앵글에서는 Student Model이 우선 작동하여 더 정교한 결과를 보여줍니다.

0. 사전 준비 (최초 1회)

모델 다운로드: 필수 AI 모델 파일들을 받습니다.

python tools/download_models.py


스튜디오 실행: 모든 작업은 스튜디오에서 진행됩니다.

python tools/muse_studio.py


1. 데이터 수집 (Studio Wizard)

스튜디오가 실행되면 단계별 마법사가 나타납니다.

Step 1: 프로파일 선택

새 프로파일 만들기: 원하는 이름(예: main_cam, side_view)을 입력하고 생성합니다.

Tip: 카메라 앵글마다 다른 이름을 써야 합니다.

기존 프로파일: 이미 만든 프로파일을 선택하고 Append(데이터 추가) 또는 Reset(새로 시작)을 선택합니다.

Step 2: 카메라 연결

사용할 웹캠을 리스트에서 선택하고 [연결] 버튼을 누릅니다.

화면이 나오면 **[다음]**으로 넘어갑니다.

Step 3: 녹화 (Data Collection)

이 단계가 가장 중요합니다. 데이터의 질이 AI 성능을 결정합니다.

배경 촬영 (필수):

카메라 앞에서 비키세요. 빈 배경만 보여야 합니다.

키보드 B 키를 누르거나 [빈 배경 촬영하기] 버튼을 클릭하세요.

주의: 조명이나 의자 위치가 바뀌면 다시 찍어야 합니다.

데이터 녹화:

[녹화 시작] 버튼을 누릅니다.

평소 방송하듯이 움직이세요. 다양한 옷을 입고, 다양한 포즈를 취하면 좋습니다.

권장량:

최소: 1분 (약 300~500장)

권장: 3~5분 (약 1500장 이상)

충분히 찍었다면 [녹화 중지] 후 **[다음: AI 학습 시작하기]**로 이동합니다.

2. AI 자동 학습 (Auto Training)

스튜디오의 마지막 탭에서 학습을 관리합니다.

1단계: 분석 (Analyze)

[1단계: 영상 분석 시작] 버튼을 누릅니다.

시스템이 녹화된 영상을 스캔하고, SAM 2.1을 이용해 샘플을 분석합니다.

분석이 끝나면 녹화된 영상 목록과 미리보기(썸네일)가 뜹니다.

2단계: 학습 및 변환 (Train & Convert)

학습에 포함시킬 영상들을 체크합니다 (보통 모두 선택).

[2단계: 학습 시작] 버튼을 누릅니다.

진행 과정 (자동):

Labeling: Teacher AI가 영상을 프레임 단위로 분석해 정답지를 만듭니다.

Filter: 품질이 낮은 데이터(사람이 없거나 흐릿한 것)를 자동 제거합니다.

Training: Seg(배경) 모델과 Pose(뼈대) 모델을 각각 학습합니다.

Conversion: 학습된 .pth 파일을 실시간용 .engine 파일로 변환합니다.

소요 시간: RTX 3060 기준, 영상 3분 분량 학습에 약 20~40분 소요됩니다.

3. 방송 시작 (Live Streaming)

학습이 완료되었다면 스튜디오를 닫고 실제 방송용 엔진을 켭니다.

python src/main.py


(또는 tools/run_muse.py 실행)

엔진 작동 확인

Launcher: 시작 시 사용할 프로파일을 선택합니다.

Hybrid Mode 확인:

콘솔 로그에 [BRAIN] Switched to: {프로파일명} 메시지가 뜨면 학습된 모델이 로드된 것입니다.

학습된 모델이 로드되면 배경 경계선이 훨씬 선명하고 움직임이 부드러워집니다.

만약 학습된 모델이 없다면 자동으로 기본 모델(MODNet)이 작동합니다.

실시간 단축키

B: 배경 리셋 (조명이 바뀌었을 때 필수)

프로파일 단축키: src/ui/launcher.py에서 설정한 단축키로 카메라 앵글과 AI 모델을 즉시 전환할 수 있습니다.