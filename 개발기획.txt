[Project MUSE] Master Plan: The Visual Singularity

Version: 6.5 (MediaPipe Integration)

Target:

Training Environment: RTX 4090 (Offline / Night Time)

Streaming Environment: RTX 3060 ~ 4060 Ti (Real-time Broadcasting)

Core Philosophy: "Train Heavy, Run Light. Zero Distortion."

프로젝트 개요 (Executive Summary)

1.1 프로젝트 정의
MUSE는 '범용적인 무거운 AI'를 사용하는 기존 방식에서 탈피하여, 사용자의 방(Background)과 신체(Body)에 완벽하게 과적합(Overfitting)된 '개인화 경량 모델'을 즉석에서 생성하는 차세대 방송 솔루션입니다.
궁극적으로는 실사(Phase 1), 버튜버(Phase 2), AI 아바타(Phase 3)를 아우르는 통합 비주얼 플랫폼을 지향합니다.

1.2 핵심 가치 (Core Values)
Resource Dominance (자원 독점): 송출컴의 GPU는 오직 MUSE의 렌더링을 위해 존재합니다.
Smart Optimization (지능형 최적화): '지식 증류(Knowledge Distillation)'를 통해 RTX 3060에서도 4K 60fps 방어를 실현합니다.
Zero Distortion (배경 왜곡 제로): 인물과 배경을 완벽히 분리(Segmentation)한 뒤 빈 공간을 채우는(Inpainting) 방식으로 물리적 왜곡을 원천 차단합니다.

1.3 하드웨어 요구사항 (Dual Track)
[Track A: Training Machine] (방송 송출용 PC와 같을 수도, 다를 수도 있음)

GPU: RTX 4090 or 3090 (24GB VRAM 권장)

역할: SAM ViT-Huge + ViTPose 구동, 고속 학습(Training)

[Track B: Streaming Machine] (실제 방송 송출)

GPU: RTX 3060 12GB 이상

역할: 경량화된 Student Model 추론 (Inference), MediaPipe Face Tracking, OBS 송출

핵심 파이프라인: Hybrid Distillation

2.1 The Teachers (Offline / High-Spec)
세상에서 가장 똑똑하지만 느린 모델들이 '정답지(Label)'를 만듭니다.

Teacher A (Segmentation): SAM (ViT-Huge). 머리카락 한 올, 옷 주름까지 완벽하게 분리.

Teacher B (Body Keypoints): ViTPose-Huge. 관절 위치를 픽셀 단위로 추적하여 SAM에게 힌트(Prompt) 제공.

2.2 The Student & Face Engine (Online / Low-Latency)
선생님들이 만든 정답지를 보고 '내 방'과 '내 모습'만 달달 외웁니다.

Face Engine: MediaPipe Face Mesh. (InsightFace 대체)

장점: CPU 기반 초경량 추론, 478개 정밀 랜드마크 지원.

역할: 눈, 코, 입, 턱선의 미세한 움직임 및 표정 추적.

Student Model: MobileNetV3-UNet 또는 EfficientNet-Lite 기반.

역할: 실시간 인물/배경 분리 (Segmentation).

단계별 상세 기능 명세 (Detailed Roadmap)

[Phase 1] MUSE v1.0 : Personalized Anti-Distortion (배경 왜곡 완전 해결)
목표: "현존하는 그 어떤 툴보다 압도적으로 예쁘고 자연스러운 실사 화면 구현"

3.1 Data Acquisition (데이터 수집기)

Clean Plate Capture: 사용자가 없는 '빈 방' 이미지를 확보하여 Inpainting 소스로 활용.

Action Recording: 다양한 조명, 각도, 거리에서 사용자의 움직임을 10분간(약 3만 프레임) 녹화.

3.2 Auto-Labeling System (자동 라벨링)

ViTPose가 사람의 관절 좌표를 추출 -> SAM에게 "이 좌표에 있는 덩어리를 따줘"라고 요청.

SAM이 배경을 완벽하게 날린 'Alpha Mask' 생성.

3.3 Real-time Rendering (Inference)

Face Warping: MediaPipe 랜드마크를 기반으로 눈 크기, 턱선 보정 수행.

Body Warping: ViTPose 기반 허리/어깨 보정.

Background Reconstruction: 인물이 줄어들어 생긴 빈 공간에 'Clean Plate'를 합성.

[Phase 2] MUSE v2.0 : The Cinematic VTuber
목표: "VTube Studio 완전 대체 및 Cinema 4D 수준의 렌더링 품질 제공"

3.4 Advanced Rendering Engine

Format: VRM 표준 지원 및 High-Poly FBX 지원.

Real-time Ray Tracing (RTX On):

캐릭터 눈동자에 비치는 반사광(Reflection).

금속 장신구, 라텍스 의상의 질감 표현.

실시간 앰비언트 오클루전(AO)으로 깊이감 있는 그림자 생성.

True Physics Simulation:

단순 스프링 본(Spring Bone)이 아닌 물리 엔진(PhysX/Bullet) 기반 시뮬레이션.

머리카락이 어깨에 닿으면 자연스럽게 흘러내리고, 가슴과 의상의 충돌 처리를 완벽하게 구현.

3.5 Hybrid Overlay

크로마키 없이 캡쳐보드 게임 화면(Background) 위에 캐릭터를 합성.

Screen-Space Reflection: 게임 화면의 불꽃이 튀면 캐릭터 얼굴에도 붉은 빛이 반사됨.

[Phase 3] MUSE v3.0 : The Genesis (Generative AI)
목표: "매일 얼굴과 의상을 바꾸는 제3의 인격(Hyper-Real Avatar) 생성"

3.6 Real-time Diffusion Pipeline

Engine: Stable Diffusion XL (SDXL) Turbo 또는 Flux.1 (TensorRT Optimized).

ControlNet Integration: Core Engine에서 추출한 ViTPose(뼈대)와 Depth Map을 입력으로 사용.

Feature:

Instant Cosplay: 프롬프트에 "White Wedding Dress" 입력 시 1초 만에 의상 변경.

Consistency Control: AnimateDiff 기술을 응용하여 움직임 시 발생하는 텍스처 떨림(Flickering) 억제.

3.7 Style Transfer

실사 모드(v1.0) 방송 중 특정 이벤트 발생 시, 화면 전체를 '유화 스타일'이나 '사이버펑크 스타일'로 실시간 필터링.

기술 스택 상세 (Updated)

4.1 Core & AI

Teacher Models: Segment Anything (ViT-H), ViTPose-Huge (TensorRT)

Face Engine: MediaPipe Face Mesh (CPU Optimized)

Student Model: PyTorch -> ONNX -> TensorRT (FP16 Quantization)

Training: Mixed Precision Training (AMP) for Speed

4.2 Application

Language: Python 3.10

GUI: PySide6

Graphics: OpenGL 4.6 (ModernGL)

초기 개발 마일스톤 (Revised)

Week 1: [Tool] 'tools/auto_labeling' 구현. SAM + ViTPose 연동 테스트.
Week 2: [Pipeline] 데이터 수집(Recorder) 및 자동 라벨링 파이프라인 구축.
Week 3: [Training] 경량 모델(UNet 계열) 선정 및 학습 루프 구현.
Week 4: [Inference] TensorRT 변환 및 실시간 렌더러 연동.