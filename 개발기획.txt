[Project MUSE] Master Plan: The Visual Singularity

Version: 9.0 (The Self-Healing Zero Distortion Architecture)

Target:
Training Environment: RTX 4090 (Offline / Night Time)
Streaming Environment: RTX 3060 ~ 4060 Ti (Real-time Broadcasting)

Core Philosophy: "Train Heavy, Run Light. Zero Distortion."

프로젝트 개요 (Executive Summary)

1.1 프로젝트 정의
MUSE는 기존의 단순한 '보정 필터'나 '버튜버 툴'을 넘어선 High-End Virtual Broadcasting Station입니다.
'범용 AI'의 비효율성을 탈피하여 사용자에게 과적합(Overfitting)된 '개인화 모델'을 생성하고, 이를 기반으로 실사(Phase 1), 버튜버(Phase 2), AI 아바타(Phase 3)를 아우르는 통합 비주얼 플랫폼을 구축합니다.

1.2 핵심 가치 (Core Values)
Resource Dominance (자원 독점): 송출컴의 GPU는 오직 MUSE의 렌더링을 위해 존재합니다. 최적화를 위해 화질을 희생하지 않습니다.
Smart Optimization (지능형 최적화): '지식 증류(Knowledge Distillation)'를 통해 RTX 3060에서도 4K 60fps 방어를 실현합니다.
Zero Distortion (배경 왜곡 제로): 인물과 배경을 완벽히 분리(Segmentation)한 뒤, 실시간으로 업데이트되는 배경 레이어와 합성(Compositing)하여 물리적 왜곡을 원천 차단합니다.
One-Core Ecosystem (단일 코어 생태계): 단 하나의 트래킹 엔진으로 실사 보정부터 버튜버, AI 아바타까지 모든 모드를 통합 제어합니다.

1.3 하드웨어 요구사항 (Dual Track)
[Track A: Training Machine] (방송 송출용 PC와 같을 수도, 다를 수도 있음)
GPU: RTX 4090 or 3090 (24GB VRAM 권장)
역할: SAM ViT-Huge + ViTPose 구동, 고속 학습(Training)

[Track B: Streaming Machine] (실제 방송 송출)
GPU: RTX 3060 12GB 이상
역할: 경량화된 Student Model 추론 (Inference), OBS 송출

핵심 파이프라인: Teacher-Student Distillation (Updated)

2.1 The Teachers (Offline / High-Spec)
세상에서 가장 똑똑하지만 느린 모델들이 '정답지(Label)'를 만듭니다. 방송 중에는 사용되지 않습니다.
Teacher A (Segmentation): SAM (ViT-Huge). 머리카락 한 올, 옷 주름, 손가락 사이까지 완벽하게 분리.
Teacher B (Keypoints): ViTPose-Huge. 관절 위치를 픽셀 단위로 추적하여 SAM에게 힌트(Prompt) 제공 및 Student 모델의 뼈대 학습 지도.

2.2 The Student (Online / Low-Latency)
선생님들이 만든 정답지를 보고 '내 방'과 '내 모습'만 달달 외운 "개인화된 천재"입니다.
Model: MobileNetV3 기반 Multi-Task Model (2 Heads: Segmentation + Pose).
Input: 사용자 개인화 데이터 (10분 영상, 약 3,600장).
Role:
배경 분리 (Segmentation): "이 픽셀이 주인님인가 배경인가?"를 0.005초 안에 판단.
뼈대 추적 (Pose Estimation): 성형을 위한 신체 좌표 제공 (VitPose 대체).
Performance: 4K 해상도에서 100fps 이상 처리 속도 확보.

단계별 상세 기능 명세 (Detailed Roadmap)

[Phase 1] MUSE v1.0 : Personalized Anti-Distortion (배경 왜곡 완전 해결)
목표: "현존하는 그 어떤 툴보다 압도적으로 예쁘고 자연스러운 실사 화면 구현"

3.1 Data Acquisition (데이터 수집)
Recorder Tool: tools/recorder.py를 통해 데이터 수집.
Base Clean Plate: 초기값으로 사용할 '빈 방' 이미지 확보.
Motion Data Capture: 다양한 자세(가림, 격한 움직임 포함)의 10분 영상 녹화.
Strategy: 3~5분의 '가림(Occlusion)' 데이터를 포함하여 손이 얼굴을 가려도 배경으로 인식되지 않도록 학습.

3.2 Auto-Labeling System (자동 라벨링)
ViTPose가 사람의 코/목 좌표를 추출 -> SAM에게 "이 좌표에 있는 덩어리를 따줘"라고 요청.
SAM이 배경을 완벽하게 날린 'Alpha Mask' 생성.
사람 손을 거치지 않고 3,600장의 고품질 정예 데이터셋(Image + Mask + Keypoints) 구축.

3.3 Real-time Rendering (Hybrid Inference & Self-Healing BG)
Student Model (New): 실시간으로 인물 마스크 생성(배경 방어) 및 몸 뼈대 추적.
Dynamic Background Update (New):

실시간으로 마스크 바깥쪽(배경) 영역을 학습하여 '배경 버퍼'를 최신 상태로 유지.

조명 변화, 카메라 미세 흔들림, 배경 물체 이동을 자동으로 반영하여 '티 안 나는 합성' 구현.
Warping Logic:

인물 영역만 성형 적용.

줄어든 인물 영역의 빈 공간은 'Self-Healing 된 최신 배경'으로 메움.
-> 결과: 몸을 아무리 흔들어도 배경은 고정되어 있으며, 조명이 바뀌어도 자연스럽게 동기화됨.

[Phase 2] MUSE v2.0 : The Cinematic VTuber (복구됨)
목표: "VTube Studio 완전 대체 및 Cinema 4D 수준의 렌더링 품질 제공"

3.4 Advanced Rendering Engine
Format: VRM 표준 지원 및 High-Poly FBX 지원.
Real-time Ray Tracing (RTX On):

캐릭터 눈동자에 비치는 반사광(Reflection).

금속 장신구, 라텍스 의상의 질감 표현.

실시간 앰비언트 오클루전(AO)으로 깊이감 있는 그림자 생성.
True Physics Simulation:

단순 스프링 본(Spring Bone)이 아닌 물리 엔진(PhysX/Bullet) 기반 시뮬레이션.

머리카락이 어깨에 닿으면 자연스럽게 흘러내리고, 가슴과 의상의 충돌 처리를 완벽하게 구현.

3.5 Hybrid Overlay
크로마키 없이 캡쳐보드 게임 화면(Background) 위에 캐릭터를 합성.
Screen-Space Reflection: 게임 화면의 불꽃이 튀면 캐릭터 얼굴에도 붉은 빛이 반사됨.

[Phase 3] MUSE v3.0 : The Genesis (Generative AI) (복구됨)
목표: "매일 얼굴과 의상을 바꾸는 제3의 인격(Hyper-Real Avatar) 생성"

3.6 Real-time Diffusion Pipeline
Engine: Stable Diffusion XL (SDXL) Turbo 또는 Flux.1 (TensorRT Optimized).
ControlNet Integration: Core Engine에서 추출한 ViTPose(뼈대)와 Depth Map을 입력으로 사용.
Feature:

Instant Cosplay: 프롬프트에 "White Wedding Dress" 입력 시 1초 만에 의상 변경.

Consistency Control: AnimateDiff 기술을 응용하여 움직임 시 발생하는 텍스처 떨림(Flickering) 억제.

3.7 Style Transfer
실사 모드(v1.0) 방송 중 특정 이벤트 발생 시, 화면 전체를 '유화 스타일'이나 '사이버펑크 스타일'로 실시간 필터링.

기술 스택 상세 (Updated)

4.1 Core & AI
Teacher Models: Segment Anything (ViT-H), ViTPose-Huge (TensorRT) - Offline Only
Student Model: PyTorch (MobileNetV3) -> TensorRT (FP16 Quantization) - Real-time
Training: Mixed Precision Training (AMP) for Speed

4.2 Application
Language: Python 3.10
GUI: PySide6
Graphics: OpenGL 4.6 (ModernGL) & CUDA (CuPy)
I/O: NVIDIA Video Codec SDK (NVDEC/NVENC)

초기 개발 마일스톤 (Revised)

Week 1: [Data] recorder.py로 데이터 수집 및 auto_labeler로 정답지(Label) 생성.
Week 2: [Training] Multi-Task Student 모델 학습 (Segmentation + Pose).
Week 3: [Inference] Student 모델 실시간 추론기 구현 및 ViTPose 대체.
Week 4: [Integration] Self-Healing Background 엔진(v12.0) 구현 및 최종 통합.