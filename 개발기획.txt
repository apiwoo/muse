[Project MUSE] Master Plan: The Visual Singularity

Version: 10.0 (The Multi-Cam Broadcasting Station)

Target:
Training Environment: RTX 4090 (Offline / Night Time)
Streaming Environment: RTX 3060 ~ 4060 Ti (Real-time Broadcasting)

Core Philosophy: "Hyper-Scale Performance on Consumer Hardware. Zero Server Cost."

1. 프로젝트 개요 (Executive Summary)

1.1 프로젝트 정의
MUSE는 단순한 보정 툴이 아닌, 개인 PC 한 대로 방송국 중계차 수준의 **'멀티 캠 가상 스튜디오'**를 구축하는 솔루션입니다.
사용자에게 과적합(Overfitting)된 **'카메라별 전용 모델(Profiled Models)'**을 생성하여, 어떤 앵글에서도 완벽한 배경 분리와 성형을 제공합니다.

1.2 핵심 가치 (Core Values)

Multi-View Mastery: 정면, 항공, 측면 등 각 카메라 앵글마다 별도의 AI 두뇌를 가집니다. 범용 모델의 어설픔을 거부합니다.

Instant Switching: 카메라 전환 시 AI 모델과 배경 데이터도 0.1초 내에 동기화되어 전환됩니다.

On-Device Neural Compression: 고성능 지식 증류를 통해 RTX 3060에서도 2~3개의 모델을 동시에 운용할 수 있습니다.

Zero Distortion: 'Self-Healing Background' 기술로 카메라가 바뀌어도 배경 왜곡이 발생하지 않습니다.

1.3 하드웨어 요구사항 (Dual Track)
[Track A: Training Machine] (방송 송출용 PC와 같을 수도, 다를 수도 있음)

GPU: RTX 4090 or 3090 (24GB VRAM 권장)

역할: SAM ViT-Huge + ViTPose 구동, 고속 학습(Training)

[Track B: Streaming Machine] (실제 방송 송출)

GPU: RTX 3060 12GB 이상

역할: 경량화된 Student Model 추론 (Inference), OBS 송출

2. 핵심 파이프라인: Profile-Based Learning

2.1 Data Acquisition (Recorder v2.0)

Profile Tagging: 사용자가 녹화 시 'front', 'top' 등의 태그를 지정합니다.

Device Awareness: pygrabber를 통해 물리적 카메라 장치와 프로파일을 맵핑합니다.

Structure: 데이터는 recorded_data/personal_data/{profile_name}/ 구조로 격리되어 저장됩니다.

2.2 Parallel Training (Trainer v2.0)

Batch Processing: 학습기는 personal_data 하위의 모든 프로파일 폴더를 스캔하여 순차적으로 학습합니다.

Output: 각 프로파일별로 독립된 가중치 파일(student_{profile}.pth)이 생성됩니다.

2.3 Real-time Inference (Engine v2.0)

Multi-Model Loader: 방송 시작 시 정의된 모든 엔진을 VRAM에 프리로딩(Pre-loading)합니다.

Context Switching: 사용자가 카메라를 전환하면 Input Source, AI Model, Background Buffer가 한 세트로 즉시 교체됩니다.

3. 단계별 상세 기능 명세 (Detailed Roadmap)

[Phase 1] MUSE v1.0 : Multi-Cam Foundation
목표: "카메라를 바꿔도 AI가 따라오는 방송 환경 구축"

3.1 Smart Recorder

기능: Global Reset / Fine-tune 모드 지원.

UX: 카메라 장치명 표시 및 직관적인 프로파일 관리.

Logic: background.jpg (Clean Plate)와 train_video_XX.mp4를 프로파일 폴더에 자동 분류 저장.

3.2 Auto-Labeling System

기능: 프로파일별 데이터셋 자동 구축 (Append 모드 지원).

성능: ViTPose-Huge(Teacher)를 활용한 초정밀 뼈대 추출 + SAM(Teacher)을 이용한 무결점 마스킹.

Output: images/, masks/, labels/ (JSON)

3.3 Real-time Rendering (Self-Healing BG)

기능: 프로파일별 배경 버퍼 독립 운영.

Logic: 카메라가 바뀔 때마다 해당 앵글의 Clean Plate를 즉시 로드하여 워핑으로 인한 빈 공간을 메꿈.

성능: RTX 3060 기준 4K 60fps 방어 (TensorRT FP16 가속).

[Phase 2] MUSE v2.0 : The Cinematic VTuber (Planned)

기능: VRM 아바타 연동 및 물리 엔진 적용.

확장: 카메라 앵글에 따른 3D 아바타 퍼스펙티브 자동 보정 (Top View에서는 정수리가 보이도록).

[Phase 3] MUSE v3.0 : The Genesis (Planned)

기능: 실시간 생성형 AI(SDXL/Flux) 기반 의상/스타일 변경.

연동: Multi-ControlNet을 활용하여 각 카메라 앵글에 맞는 이미지 생성.

4. 기술 스택 (Updated)

4.1 Core & AI

Source Models: Segment Anything (ViT-H), ViTPose-Huge (TensorRT) - Offline Only

Student Engine: MobileNetV3 (Multi-Head) -> TensorRT 10.x - Real-time

Training: Mixed Precision Training (AMP) for Speed

Device Control: OpenCV + pygrabber (DirectShow)

4.2 Application

Language: Python 3.10

GUI: PySide6 (Modern Dark Theme)

Graphics: OpenGL 4.6 (ModernGL) & CUDA (CuPy) Direct Processing

I/O: NVIDIA Video Codec SDK (NVDEC/NVENC)

5. 초기 개발 마일스톤 (Revised)

Week 1: [Data & Pipeline]

recorder.py 고도화 (멀티 프로파일 지원)

auto_labeling 배치 처리 구현

Week 2: [Training & Optimization]

Multi-Task Student 모델 학습 (Segmentation + Pose)

TensorRT 변환 파이프라인 구축 (convert_student_to_trt.py)

Week 3: [Inference & UX]

실시간 엔진(run_muse.py) 멀티 모델 로딩 구현

카메라 스위칭 로직 및 배경 리셋(B키) 연동

Week 4: [Integration]

최종 통합 테스트 및 성능 최적화 (Memory Leak Check)