[Project MUSE] Master Plan: The Visual Singularity

Version: 7.0 (The Definitive Integrated Master Plan)

Target:

Training Environment: RTX 4090 (Offline / Night Time)

Streaming Environment: RTX 3060 ~ 4060 Ti (Real-time Broadcasting)

Core Philosophy: "Train Heavy, Run Light. Zero Distortion."

프로젝트 개요 (Executive Summary)

1.1 프로젝트 정의
MUSE는 기존의 단순한 '보정 필터'나 '버튜버 툴'을 넘어선 High-End Virtual Broadcasting Station입니다.
'범용 AI'의 비효율성을 탈피하여 사용자에게 과적합(Overfitting)된 '개인화 모델'을 생성하고, 이를 기반으로 실사(Phase 1), 버튜버(Phase 2), AI 아바타(Phase 3)를 아우르는 통합 비주얼 플랫폼을 구축합니다.

1.2 핵심 가치 (Core Values)
Resource Dominance (자원 독점): 송출컴의 GPU는 오직 MUSE의 렌더링을 위해 존재합니다. 최적화를 위해 화질을 희생하지 않습니다.
Smart Optimization (지능형 최적화): '지식 증류(Knowledge Distillation)'를 통해 RTX 3060에서도 4K 60fps 방어를 실현합니다.
Zero Distortion (배경 왜곡 제로): 인물과 배경을 완벽히 분리(Segmentation)한 뒤 빈 공간을 채우는(Inpainting) 방식으로 물리적 왜곡을 원천 차단합니다.
One-Core Ecosystem (단일 코어 생태계): 단 하나의 트래킹 엔진으로 실사 보정부터 버튜버, AI 아바타까지 모든 모드를 통합 제어합니다.

1.3 하드웨어 요구사항 (Dual Track)
[Track A: Training Machine] (방송 송출용 PC와 같을 수도, 다를 수도 있음)

GPU: RTX 4090 or 3090 (24GB VRAM 권장)

역할: SAM ViT-Huge + ViTPose 구동, 고속 학습(Training)

[Track B: Streaming Machine] (실제 방송 송출)

GPU: RTX 3060 12GB 이상

역할: 경량화된 Student Model 추론 (Inference), OBS 송출

핵심 파이프라인: Teacher-Student Distillation

2.1 The Teachers (Offline / High-Spec)
세상에서 가장 똑똑하지만 느린 모델들이 '정답지(Label)'를 만듭니다.

Teacher A (Segmentation): SAM (ViT-Huge). 머리카락 한 올, 옷 주름까지 완벽하게 분리.

Teacher B (Keypoints): ViTPose-Huge. 관절 위치를 픽셀 단위로 추적하여 SAM에게 힌트(Prompt) 제공.

2.2 The Student (Online / Low-Latency)
선생님들이 만든 정답지를 보고 '내 방'과 '내 모습'만 달달 외웁니다.

Model: MobileNetV3 기반 Multi-Task Model (Mask + Pose 동시 출력).

Performance: 4K 해상도에서 100fps 이상 처리 속도 확보.

Goal: "이 픽셀이 사람인가 배경인가?"를 0.005초 안에 판단.

단계별 상세 기능 명세 (Detailed Roadmap)

[Phase 1] MUSE v1.0 : Personalized Anti-Distortion (배경 왜곡 완전 해결)
목표: "현존하는 그 어떤 툴보다 압도적으로 예쁘고 자연스러운 실사 화면 구현"

3.1 Data Acquisition (데이터 수집)

Clean Plate Capture: 사용자가 없는 '빈 방' 이미지를 확보하여 Inpainting 소스로 활용.

Smart Sampling: 10분(36,000프레임) 영상에서 '유의미한 변화'가 있는 3,000~5,000장만 추출.

움직임이 적을 때는 스킵, 클 때는 캡처하는 가변 샘플링 적용.

학습 효율 극대화 및 과적합 방지.

3.2 Auto-Labeling System (자동 라벨링)

ViTPose가 사람의 코/목 좌표를 추출 -> SAM에게 "이 좌표에 있는 덩어리를 따줘"라고 요청.

SAM이 배경을 완벽하게 날린 'Alpha Mask' 생성.

사람 손을 거치지 않고 3,000장의 고품질 정예 데이터셋 구축.

3.3 Real-time Rendering (Hybrid Inference)

Student Model: 실시간으로 인물 마스크 생성 및 배경 방어.

InsightFace: 고정밀 얼굴 좌표 추출 (디테일 담당).

Warping Logic:

마스크 안쪽(인물)만 성형 적용.

마스크 바깥쪽(배경)은 절대 건드리지 않음.

Background Reconstruction: 인물이 줄어들어 생긴 빈 공간에 'Clean Plate'를 합성.
-> 결과: 허리를 절반으로 줄여도 문틀이나 커튼이 전혀 휘지 않음.

[Phase 2] MUSE v2.0 : The Cinematic VTuber (복구됨)
목표: "VTube Studio 완전 대체 및 Cinema 4D 수준의 렌더링 품질 제공"

3.4 Advanced Rendering Engine

Format: VRM 표준 지원 및 High-Poly FBX 지원.

Real-time Ray Tracing (RTX On):

캐릭터 눈동자에 비치는 반사광(Reflection).

금속 장신구, 라텍스 의상의 질감 표현.

실시간 앰비언트 오클루전(AO)으로 깊이감 있는 그림자 생성.

True Physics Simulation:

단순 스프링 본(Spring Bone)이 아닌 물리 엔진(PhysX/Bullet) 기반 시뮬레이션.

머리카락이 어깨에 닿으면 자연스럽게 흘러내리고, 가슴과 의상의 충돌 처리를 완벽하게 구현.

3.5 Hybrid Overlay

크로마키 없이 캡쳐보드 게임 화면(Background) 위에 캐릭터를 합성.

Screen-Space Reflection: 게임 화면의 불꽃이 튀면 캐릭터 얼굴에도 붉은 빛이 반사됨.

[Phase 3] MUSE v3.0 : The Genesis (Generative AI) (복구됨)
목표: "매일 얼굴과 의상을 바꾸는 제3의 인격(Hyper-Real Avatar) 생성"

3.6 Real-time Diffusion Pipeline

Engine: Stable Diffusion XL (SDXL) Turbo 또는 Flux.1 (TensorRT Optimized).

ControlNet Integration: Core Engine에서 추출한 ViTPose(뼈대)와 Depth Map을 입력으로 사용.

Feature:

Instant Cosplay: 프롬프트에 "White Wedding Dress" 입력 시 1초 만에 의상 변경.

Consistency Control: AnimateDiff 기술을 응용하여 움직임 시 발생하는 텍스처 떨림(Flickering) 억제.

3.7 Style Transfer

실사 모드(v1.0) 방송 중 특정 이벤트 발생 시, 화면 전체를 '유화 스타일'이나 '사이버펑크 스타일'로 실시간 필터링.

기술 스택 상세 (Updated)

4.1 Core & AI

Teacher Models: Segment Anything (ViT-H), ViTPose-Huge (TensorRT)

Student Model: PyTorch (MobileNetV3) -> TensorRT (FP16 Quantization)

Training: Mixed Precision Training (AMP) for Speed

4.2 Application

Language: Python 3.10

GUI: PySide6

Graphics: OpenGL 4.6 (ModernGL)

I/O: NVIDIA Video Codec SDK (NVDEC/NVENC)

초기 개발 마일스톤 (Revised)

Week 1: [Tool] 'tools/auto_labeling' 구현. SAM + ViTPose 연동 테스트.
Week 2: [Pipeline] 데이터 수집(Recorder) 및 자동 라벨링 파이프라인 구축.
Week 3: [Training] Multi-Task Student 모델 선정 및 학습 루프 구현.
Week 4: [Inference] TensorRT 변환 및 실시간 렌더러 연동 (배경 합성 쉐이더 구현).