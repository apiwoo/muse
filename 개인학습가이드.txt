Project MUSE: The Visual Singularity

Zero Distortion Pipeline User Manual (v12.1 Edition)

🚀 1. 시스템 개요

이 시스템은 **"사용자에게 과적합(Overfitting)된 개인화 모델"**을 사용하여, 저사양 GPU에서도 완벽한 배경 분리와 신체 추적을 수행합니다.
범용성을 포기하는 대신, 특정 환경(내 방)과 특정 인물(나)에 대해서는 선생님 모델(ViTPose/SAM) 수준의 성능을 냅니다.

[v12.1 업데이트: TensorRT 가속 & 배경 리셋]
이제 학습된 모델을 TensorRT 엔진으로 변환하여 RTX 3060에서도 4K 60fps 방어가 가능합니다.
또한 방송 중 조명이 바뀌거나 카메라가 흔들려도 키보드 단축키 'B' 하나로 배경을 즉시 리셋할 수 있습니다.

🎥 2. 파이프라인 단계별 가이드

[Step 1] 데이터 수집 (Recording)

목표: 학습에 필요한 "내 모습"과 "초기 배경" 데이터를 확보합니다.

실행: python tools/recorder.py

초기 배경 촬영 (Base Clean Plate):

화면 밖으로 나가세요.

B 키를 눌러 background.jpg를 저장합니다.

이 사진은 시스템 시작 시 초기값으로 사용되며, 이후에는 실시간으로 업데이트됩니다.

학습 데이터 녹화:

다시 화면 안으로 들어오세요.

R 키를 눌러 녹화를 시작합니다.

약 5~10분간 다양한 동작을 수행하세요.

가만히 서 있기, 앉기, 팔 벌리기

중요: 손으로 얼굴 가리기, 몸 돌리기 등 "AI가 헷갈릴 만한 동작"을 많이 넣어야 강해집니다.

다시 R을 눌러 저장하고 종료(Q)합니다.

[Step 2] 정답지 생성 (Auto-Labeling)

목표: 녹화된 영상에 대해 선생님 모델들이 "정답(Mask, Keypoints)"을 달아줍니다.

실행: python tools/auto_labeling/run_labeling.py <SESSION_NAME>

예: python tools/auto_labeling/run_labeling.py 20231025_143000

작동 원리:

Teacher A (ViTPose-Huge): 뼈대 좌표를 아주 정밀하게 찾습니다.

Teacher B (SAM ViT-Huge): 뼈대 좌표를 힌트로 받아, 사람 영역(Mask)을 칼같이 따냅니다.

결과: recorded_data/<SESSION>/ 폴더에 images, masks, labels가 생성됩니다.

[Step 3] 개인화 학습 (Distillation Training)

목표: 선생님의 지식을 경량 모델(Student)에게 주입합니다.

실행: python tools/train_student.py <SESSION_NAME>

과정:

MobileNetV3 기반의 학생 모델이 데이터를 보며 학습합니다.

입력: 내 방에서 찍은 내 사진

출력: 배경 제거 마스크 + 뼈대 좌표

완료: 약 30분~1시간 후 assets/models/personal/student_model_final.pth가 생성됩니다.

[Step 4] 최적화 (Optimization) - [New & Critical]

목표: 학습된 PyTorch 모델(.pth)을 **초고속 TensorRT 엔진(.engine)**으로 변환합니다.
이 과정을 거치지 않으면 추론 속도가 느려 4K 60fps를 방어할 수 없습니다.

실행: python tools/convert_student_to_trt.py

과정:

PyTorch 모델을 ONNX 포맷으로 변환합니다.

ONNX 모델을 NVIDIA TensorRT 엔진으로 컴파일(FP16 양자화 포함)합니다.

결과: assets/models/personal/student_model.engine 파일이 생성됩니다.

이제 시스템은 자동으로 .pth 대신 .engine을 우선 로드하여 가동됩니다.

[Step 5] 엔진 구동 (Live Inference)

목표: 최적화된 모델을 로드하여 실시간 방송을 시작합니다.

실행: python tools/run_muse.py (또는 src/main.py)

작동 로직 (Zero Distortion Pipeline):

Input: 웹캠 영상 (Live)

AI Inference: Student 엔진(TRT)이 실시간으로 뼈대와 배경 마스크를 추출합니다.

Background Update: 마스크 바깥쪽(배경) 영역을 실시간으로 학습하여 배경 버퍼를 최신 상태로 유지합니다.

Rendering:

뼈대를 기준으로 몸을 성형(Warping)합니다.

줄어든 공간을 최신 업데이트된 배경 버퍼로 메꿉니다(Compositing).

⚠️ 3. 실시간 대응 (Live Ops)

배경 즉시 리셋 (Manual Reset) - [New]

상황: 방송 중 조명을 확 켰거나, 의자를 치워서 배경이 달라졌는데 잔상이 남을 때.

해결: 키보드 B 키를 누르세요.

동작: 현재 프레임의 배경을 강제로 캡처하여 배경 버퍼를 즉시 덮어씌웁니다.

자가 치유 (Self-Healing)

상황: 카메라가 미세하게 흔들림.

해결: 가만히 두거나 몸을 조금 움직이면 AI가 배경을 야금야금 수정하여 복구합니다.

💡 4. 개발자 팁 (Optimization)

추론 속도: TensorRT 변환 후 RTX 3060 기준 4ms 내외로 추론이 끝납니다. 남는 GPU 자원은 고사양 게임 송출에 쓰세요.

재학습: 옷이 바뀌거나 계절이 바뀌면 Step 1부터 3까지 반복하여 새로운 모델을 만드세요. Step 4(변환)도 잊지 말고 다시 실행해야 합니다.