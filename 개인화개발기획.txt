[Project MUSE] Master Plan: The Visual Singularity

Version: 10.2 (Localization & UX Update)

Target:
Training Environment: RTX 4090 (Offline / Night Time)
Streaming Environment: RTX 3060 ~ 4060 Ti (Real-time Broadcasting)

Core Philosophy: "Hyper-Scale Performance on Consumer Hardware. Zero Server Cost."

프로젝트 개요 (Executive Summary)

1.1 프로젝트 정의
MUSE는 단순한 보정 툴이 아닌, 개인 PC 한 대로 방송국 중계차 수준의 **'멀티 캠 가상 스튜디오'**를 구축하는 솔루션입니다.
사용자에게 과적합(Overfitting)된 **'카메라별 전용 모델(Profiled Models)'**을 생성하여, 어떤 앵글에서도 완벽한 배경 분리와 성형을 제공합니다.

1.2 핵심 가치 (Core Values)

Multi-View Mastery: 정면, 항공, 측면 등 각 카메라 앵글마다 별도의 AI 두뇌를 가집니다. 범용 모델의 어설픔을 거부합니다.

Instant Switching: 카메라 전환 시 AI 모델과 배경 데이터도 0.1초 내에 동기화되어 전환됩니다.

On-Device Neural Compression: 고성능 지식 증류를 통해 RTX 3060에서도 2~3개의 모델을 동시에 운용할 수 있습니다.

Zero Distortion: 'Self-Healing Background' 기술로 카메라가 바뀌어도 배경 왜곡이 발생하지 않습니다.

User-Centric Localization: 모든 UI/UX를 직관적인 한국어로 구성하여, 영어를 모르는 사용자도 매뉴얼 없이 즉시 사용할 수 있습니다.

1.3 하드웨어 요구사항 (Dual Track)
[Track A: Training Machine] (방송 송출용 PC와 같을 수도, 다를 수도 있음)

GPU: RTX 4090 or 3090 (24GB VRAM 권장)

역할: SAM ViT-Huge + ViTPose 구동, 고속 학습(Training)

[Track B: Streaming Machine] (실제 방송 송출)

GPU: RTX 3060 12GB 이상

역할: 경량화된 Student Model 추론 (Inference), OBS 송출

핵심 파이프라인: Profile-Based Learning

2.1 Data Acquisition (Recorder v2.0)

Profile Tagging: 사용자가 녹화 시 'front', 'top' 등의 태그를 지정합니다.

Device Awareness: pygrabber를 통해 물리적 카메라 장치와 프로파일을 맵핑합니다.

Structure: 데이터는 recorded_data/personal_data/{profile_name}/ 구조로 격리되어 저장됩니다.

2.2 Parallel Training (Trainer v2.0)

Batch Processing: 학습기는 personal_data 하위의 모든 프로파일 폴더를 스캔하여 순차적으로 학습합니다.

Output: 각 프로파일별로 독립된 가중치 파일(student_{profile}.pth)이 생성됩니다.

2.3 Real-time Inference (Engine v2.0)

Multi-Model Loader: 방송 시작 시 정의된 모든 엔진을 VRAM에 프리로딩(Pre-loading)합니다.

Context Switching: 사용자가 카메라를 전환하면 Input Source, AI Model, Background Buffer가 한 세트로 즉시 교체됩니다.

단계별 상세 기능 명세 (Detailed Roadmap)

[Phase 1] MUSE v1.0 : Multi-Cam Foundation
목표: "카메라를 바꿔도 AI가 따라오는 방송 환경 구축"

3.1 Smart Recorder (Studio GUI Integration)

기능: MUSE Studio를 통한 원스톱 녹화 및 관리.

UX: 100% 한국어 인터페이스 적용. 복잡한 용어(Append, Reset 등)를 친숙한 표현으로 대체.

Logic: background.jpg (Clean Plate)와 train_video_XX.mp4를 프로파일 폴더에 자동 분류 저장.

3.2 Auto-Labeling System

기능: 프로파일별 데이터셋 자동 구축 (Append 모드 지원).

성능: ViTPose-Huge(Teacher)를 활용한 초정밀 뼈대 추출 + SAM(Teacher)을 이용한 무결점 마스킹.

Output: images/, masks/, labels/ (JSON)

3.3 Real-time Rendering (Self-Healing BG)

기능: 프로파일별 배경 버퍼 독립 운영.

Logic: 카메라가 바뀔 때마다 해당 앵글의 Clean Plate를 즉시 로드하여 워핑으로 인한 빈 공간을 메꿈.

성능: RTX 3060 기준 4K 60fps 방어 (TensorRT FP16 가속).

[Phase 2] MUSE v2.0 : The Cinematic VTuber (Planned)

기능: VRM 아바타 연동 및 물리 엔진 적용.

확장: 카메라 앵글에 따른 3D 아바타 퍼스펙티브 자동 보정 (Top View에서는 정수리가 보이도록).

[Phase 3] MUSE v3.0 : The Genesis (Planned)

기능: 실시간 생성형 AI(SDXL/Flux) 기반 의상/스타일 변경.

연동: Multi-ControlNet을 활용하여 각 카메라 앵글에 맞는 이미지 생성.

기술 스택 (Updated)

4.1 Core & AI

Source Models: Segment Anything (ViT-H), ViTPose-Huge (TensorRT) - Offline Only

Student Engine: MobileNetV3 (Multi-Head) -> TensorRT 10.x - Real-time

Training: Mixed Precision Training (AMP) for Speed

Device Control: OpenCV + pygrabber (DirectShow)

4.2 Application

Language: Python 3.10

GUI: PySide6 (Modern Dark Theme) - [Critical] 100% Korean Localization

Graphics: OpenGL 4.6 (ModernGL) & CUDA (CuPy) Direct Processing

I/O: NVIDIA Video Codec SDK (NVDEC/NVENC)

초기 개발 마일스톤 (Revised)

Week 1: [Data & Pipeline]

recorder.py 고도화 (멀티 프로파일 지원)

auto_labeling 배치 처리 구현

Week 2: [Training & Optimization]

Multi-Task Student 모델 학습 (Segmentation + Pose)

TensorRT 변환 파이프라인 구축 (convert_student_to_trt.py)

Week 3: [Inference & UX]

실시간 엔진(run_muse.py) 멀티 모델 로딩 구현

카메라 스위칭 로직 및 배경 리셋(B키) 연동

Week 4: [Integration]

최종 통합 테스트 및 성능 최적화 (Memory Leak Check)

배포 및 패키징 계획 (Deployment Strategy) - [Updated]

6.1 배포 철학: "Pro-Level UX & Self-Contained"

목표: 최종 사용자는 Setup.exe 실행만으로 모든 환경(Python, CUDA Libs 등)이 자동 구성됨.

방식: Portable 엔진을 인스톨러(Installer)로 감싸서 배포.

6.2 설치 구조 (Installed Structure)
C:/Program Files/MUSE_Studio/
├── MUSE.exe          # 메인 실행 파일 (Launcher)
├── uninstall.exe     # 제거 프로그램
├── _internal/        # Python 런타임 및 라이브러리 (PyTorch, Numpy 등)
├── assets/           # AI 모델(.engine), 쉐이더(.glsl), 설정 파일
└── libs/             # [Core] NVIDIA 핵심 DLL 모음 (cuDNN, TensorRT)

6.3 빌드 프로세스 (Build Pipeline)
Step 1. DLL 수집: tools/collect_libs.py 실행 -> 시스템 내 흩어진 NVIDIA DLL을 libs/로 수집.
Step 2. 코드 프리징: PyInstaller를 사용하여 Python 스크립트를 단일 폴더 방식의 .exe로 변환.
Step 3. 리소스 병합: assets 및 libs 폴더를 빌드 결과물에 포함.
Step 4. 인스톨러 생성: Inno Setup 스크립트(.iss)를 컴파일하여 MUSE_Setup_v1.0.exe 생성.

기능: 바탕화면 바로가기 생성, 시작 메뉴 등록, 관리자 권한 요청.

6.4 실행 메커니즘 (Runtime Logic)

src/utils/cuda_helper.py가 프로그램 시작 시 libs/ 폴더를 감지.

시스템 환경 변수(PATH)의 최우선 순위로 libs/를 등록.

외부 환경(사용자의 잘못된 CUDA 버전 등)에 영향받지 않고 내장된 라이브러리로 구동.