# Project MUSE - run_labeling.py
# The Teacher's Workshop: Smart Auto-Labeling (Append Support)
# (C) 2025 MUSE Corp. All rights reserved.

import os
import sys
import cv2
import numpy as np
import torch
import json
import glob
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ í™•ë³´
root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
sys.path.append(root_dir)
sys.path.append(os.path.join(root_dir, "src")) # [Fix] src í´ë”ë¥¼ ê²½ë¡œì— ì¶”ê°€í•˜ì—¬ ai ëª¨ë“ˆ ì¸ì‹

from ai.tracking.vitpose_trt import VitPoseTrt
from ai.distillation.teacher.sam_wrapper import Sam2VideoWrapper

class AutoLabeler:
    def __init__(self, root_session="personal_data"):
        self.root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.root_data_dir = os.path.join(self.root_dir, "recorded_data", root_session)
        
        if not os.path.exists(self.root_data_dir):
            print("âŒ ë°ì´í„° í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤.")
            self.profiles = []
            return
            
        self.profiles = [d for d in os.listdir(self.root_data_dir) if os.path.isdir(os.path.join(self.root_data_dir, d))]

        print("ðŸ‘¨â€ðŸ« [Teacher B] ViTPose (Keypoints) Loading...")
        self.pose_model = VitPoseTrt(engine_path=os.path.join(self.root_dir, "assets/models/tracking/vitpose_huge.engine"))

        print("ðŸ‘©â€ðŸ« [Teacher A] SAM 2 (Video Segmentation) Loading...")
        self.sam_wrapper = Sam2VideoWrapper(model_root=os.path.join(self.root_dir, "assets/models/segment_anything"))

    def process_all_profiles(self):
        total_profiles = len(self.profiles)
        for i, profile in enumerate(self.profiles):
            print(f"\n==================================================")
            print(f"   Labeling Profile ({i+1}/{total_profiles}): [{profile}]")
            print(f"==================================================")
            self._process_single_profile(profile, i, total_profiles)

    def _process_single_profile(self, profile, profile_idx, total_profiles):
        profile_dir = os.path.join(self.root_data_dir, profile)
        video_paths = sorted(glob.glob(os.path.join(profile_dir, "train_video_*.mp4")))
        
        out_imgs = os.path.join(profile_dir, "images")
        out_masks = os.path.join(profile_dir, "masks")
        out_labels = os.path.join(profile_dir, "labels")
        for d in [out_imgs, out_masks, out_labels]: os.makedirs(d, exist_ok=True)

        # [Append Logic] ì´ë¯¸ ì²˜ë¦¬ëœ ë¹„ë””ì˜¤ ëª©ë¡ ë¡œë“œ
        processed_log_path = os.path.join(profile_dir, "processed_videos.json")
        processed_videos = []
        if os.path.exists(processed_log_path):
            with open(processed_log_path, "r") as f:
                processed_videos = json.load(f)
        
        # ë‹¤ìŒ ì´ë¯¸ì§€ ì¸ë±ìŠ¤ ê³„ì‚° (ì´ì–´ì“°ê¸°)
        global_idx = self._get_next_index(out_imgs)
        
        newly_processed = []

        for v_idx, video_path in enumerate(video_paths):
            vid_name = os.path.basename(video_path)
            
            # [Smart Check] ì´ë¯¸ ì²˜ë¦¬ëœ ì˜ìƒì´ë©´ ìŠ¤í‚µ
            if vid_name in processed_videos:
                # ë‹¨, ì´ë¯¸ì§€ê°€ ì‹¤ì œë¡œ ìžˆëŠ”ì§€ í™•ì¸ì€ í•„ìš”í•  ìˆ˜ ìžˆìŒ (ì—¬ê¸°ì„œëŠ” ë¡œê·¸ ì‹ ë¢°)
                print(f"   â­ï¸  Skipping processed video: {vid_name}")
                continue

            print(f"   ðŸŽ¥ Processing New Video: {vid_name}")
            
            # [GUI Log] ë¹„ë””ì˜¤ ë‹¨ìœ„ ì§„í–‰ë¥ 
            current_progress = int(((profile_idx * len(video_paths) + v_idx) / (total_profiles * len(video_paths))) * 100)
            print(f"[PROGRESS] {current_progress}")
            
            try:
                self.sam_wrapper.init_state(video_path)
            except Exception as e:
                print(f"      âŒ SAM Init Failed: {e}")
                continue

            cap = cv2.VideoCapture(video_path)
            ret, first_frame = cap.read()
            if not ret:
                cap.release()
                continue
            
            keypoints = self.pose_model.inference(first_frame)
            if keypoints is None:
                print("      âš ï¸ No pose detected in first frame.")
                cap.release()
                self.sam_wrapper.reset()
                continue

            valid_kpts = [kp[:2] for kp in keypoints if kp[2] > 0.4]
            if len(valid_kpts) < 3:
                print("      âš ï¸ Not enough keypoints.")
                cap.release()
                self.sam_wrapper.reset()
                continue

            points = np.array(valid_kpts, dtype=np.float32)
            labels = np.ones(len(points), dtype=np.int32)
            
            self.sam_wrapper.add_prompt(frame_idx=0, points=points, labels=labels)

            print("      ðŸŒŠ Propagating masks...")
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            
            video_masks = {}
            for frame_idx, obj_ids, mask_logits in self.sam_wrapper.propagate():
                mask = (mask_logits[0] > 0.0).cpu().numpy().astype(np.uint8)
                video_masks[frame_idx] = mask

            # Save Data
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            cap.set(cv2.CAP_PROP_POS_FRAMES, 0)
            curr_f_idx = 0
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break
                
                # í”„ë ˆìž„ ì €ìž¥ ì—¬ë¶€ ê²°ì • (6í”„ë ˆìž„ë‹¹ 1ìž¥ ë“± ìƒ˜í”Œë§ ê°€ëŠ¥í•˜ë‚˜, ì—¬ê¸°ì„  ì „ë¶€ ì €ìž¥í•˜ê³  Trainerì—ì„œ ì…”í”Œ)
                # ìš©ëŸ‰ ì ˆì•½ì„ ìœ„í•´ 3í”„ë ˆìž„ë‹¹ 1ìž¥ ì €ìž¥ (ì˜µì…˜)
                # ì—¬ê¸°ì„œëŠ” í’€ ë°ì´í„°ë¥¼ ìœ„í•´ ì „ë¶€ ì €ìž¥
                
                kpts = self.pose_model.inference(frame)
                mask = video_masks.get(curr_f_idx, None)
                
                if kpts is not None and mask is not None:
                    fname = f"{global_idx:06d}"
                    cv2.imwrite(os.path.join(out_imgs, f"{fname}.jpg"), frame)
                    cv2.imwrite(os.path.join(out_masks, f"{fname}.png"), mask * 255)
                    
                    y_indices, x_indices = np.where(mask > 0)
                    if len(x_indices) > 0:
                        box = [int(np.min(x_indices)), int(np.min(y_indices)), 
                               int(np.max(x_indices)), int(np.max(y_indices))]
                    else:
                        box = [0, 0, 0, 0]

                    label_data = {"keypoints": kpts.tolist(), "box": box}
                    with open(os.path.join(out_labels, f"{fname}.json"), "w") as f:
                        json.dump(label_data, f)
                    
                    global_idx += 1
                
                curr_f_idx += 1

            cap.release()
            self.sam_wrapper.reset()
            
            # ì²˜ë¦¬ ì™„ë£Œ ëª©ë¡ì— ì¶”ê°€
            newly_processed.append(vid_name)
        
        # ë¡œê·¸ ì—…ë°ì´íŠ¸
        if newly_processed:
            processed_videos.extend(newly_processed)
            with open(processed_log_path, "w") as f:
                json.dump(processed_videos, f, indent=4)
            print(f"   âœ… Added {len(newly_processed)} videos to processed log.")
        else:
            print("   âœ¨ No new videos to process.")

    def _get_next_index(self, dir_path):
        files = glob.glob(os.path.join(dir_path, "*.jpg"))
        max_idx = -1
        for f in files:
            try:
                idx = int(os.path.splitext(os.path.basename(f))[0])
                if idx > max_idx: max_idx = idx
            except: pass
        return max_idx + 1

if __name__ == "__main__":
    session = sys.argv[1] if len(sys.argv) > 1 else "personal_data"
    labeler = AutoLabeler(session)
    labeler.process_all_profiles()