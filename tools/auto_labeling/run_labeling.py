# Project MUSE - run_labeling.py
# The Teacher's Workshop: Automatic Data Annotation (Multi-Video Support)
# (C) 2025 MUSE Corp. All rights reserved.

import os
import sys
import cv2
import numpy as np
import torch
import json
import glob
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# [Teachers]
from ai.tracking.vitpose_trt import VitPoseTrt # Teacher B
try:
    from segment_anything import sam_model_registry, SamPredictor # Teacher A
except ImportError:
    print("âŒ 'segment_anything' ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤. 'pip install git+https://github.com/facebookresearch/segment-anything.git' ì‹¤í–‰ í•„ìš”")
    sys.exit(1)

class AutoLabeler:
    def __init__(self, session_name):
        """
        [ìë™ ë¼ë²¨ë§ ì‹œìŠ¤í…œ]
        ë…¹í™”ëœ ì˜ìƒì„ ë¶„ì„í•˜ì—¬ í•™ìŠµìš© ë°ì´í„°ì…‹(Image + Mask + Keypoints)ì„ ìƒì„±í•©ë‹ˆë‹¤.
        V2.0 Update: í´ë” ë‚´ ëª¨ë“  MP4 íŒŒì¼ì„ ì²˜ë¦¬í•˜ë„ë¡ ë³€ê²½.
        """
        self.root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.data_dir = os.path.join(self.root_dir, "recorded_data", session_name)
        
        # í´ë” ë‚´ ëª¨ë“  MP4 íŒŒì¼ ê²€ìƒ‰
        self.video_paths = sorted(glob.glob(os.path.join(self.data_dir, "*.mp4")))
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (Dataset Structure)
        self.out_imgs = os.path.join(self.data_dir, "images")
        self.out_masks = os.path.join(self.data_dir, "masks")
        self.out_labels = os.path.join(self.data_dir, "labels") # Keypoints JSON
        
        for d in [self.out_imgs, self.out_masks, self.out_labels]:
            os.makedirs(d, exist_ok=True)

        if not self.video_paths:
            print(f"âŒ í´ë” ë‚´ì— MP4 ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤: {self.data_dir}")
            print("   -> ì˜ìƒì„ ë…¹í™”í•˜ê±°ë‚˜ ë³µì‚¬í•´ ë„£ìœ¼ì„¸ìš”.")
            sys.exit(1)

        # 1. Load Teacher B (ViTPose - TensorRT)
        print("ğŸ‘¨â€ğŸ« [Teacher B] ViTPose(Keypoints) ë¡œë”© ì¤‘...")
        try:
            self.pose_model = VitPoseTrt(engine_path=os.path.join(self.root_dir, "assets/models/tracking/vitpose_huge.engine"))
        except Exception as e:
            print(f"âŒ ViTPose ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)

        # 2. Load Teacher A (SAM - PyTorch)
        print("ğŸ‘©â€ğŸ« [Teacher A] SAM(Segmentation) ë¡œë”© ì¤‘...")
        sam_checkpoint = os.path.join(self.root_dir, "assets/models/segment_anything/sam_vit_h_4b8939.pth")
        if not os.path.exists(sam_checkpoint):
            print(f"âŒ SAM ëª¨ë¸ ì—†ìŒ: {sam_checkpoint}")
            print("   -> 'tools/download_models.py'ë¥¼ í™•ì¸í•˜ê±°ë‚˜ ëª¨ë¸ì„ ë‹¤ìš´ë¡œë“œí•˜ì„¸ìš”.")
            sys.exit(1)
            
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
        sam.to(device=device)
        self.sam_predictor = SamPredictor(sam)
        
        print("âœ… ì„ ìƒë‹˜ë“¤ ì¤€ë¹„ ì™„ë£Œ.")

    def process(self, frame_interval=5):
        """
        :param frame_interval: 5í”„ë ˆì„ë§ˆë‹¤ 1ì¥ ì¶”ì¶œ (30fps -> 6fps)
        """
        print(f"ğŸš€ ë¼ë²¨ë§ ì‹œì‘ (ì´ {len(self.video_paths)}ê°œ ì˜ìƒ)")
        
        global_saved_count = 0
        
        for vid_idx, video_path in enumerate(self.video_paths):
            vid_name = os.path.basename(video_path)
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            print(f"\n[{vid_idx+1}/{len(self.video_paths)}] ì²˜ë¦¬ ì¤‘: {vid_name} ({total_frames} frames)")
            
            frame_idx = 0
            pbar = tqdm(total=total_frames)
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break
                
                # ì§€ì •ëœ ê°„ê²©ë§ˆë‹¤ ì¶”ì¶œ
                if frame_idx % frame_interval == 0:
                    # íŒŒì¼ëª…ì´ ê²¹ì¹˜ì§€ ì•Šê²Œ ì „ì²´ ì¹´ìš´íŠ¸ ì‚¬ìš©
                    success = self._annotate_frame(frame, global_saved_count)
                    if success:
                        global_saved_count += 1
                
                frame_idx += 1
                pbar.update(1)
                
            pbar.close()
            cap.release()
            
        print(f"\nğŸ‰ ì „ì²´ ì™„ë£Œ! ì´ {global_saved_count}ì¥ì˜ ì •ë‹µ ë°ì´í„° ìƒì„±.")
        print(f"ğŸ‘‰ ê²½ë¡œ: {self.data_dir}")

    def _annotate_frame(self, frame, idx):
        # 1. Pose Estimation (Teacher B)
        # ViTPoseëŠ” BGR ì´ë¯¸ì§€ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        keypoints = self.pose_model.inference(frame) # [17, 3] (x, y, conf)
        
        if keypoints is None:
            return False

        # ìœ íš¨í•œ í‚¤í¬ì¸íŠ¸(ì‹ ë¢°ë„ > 0.4) í•„í„°ë§
        valid_kpts = []
        for kp in keypoints:
            x, y, conf = kp
            if conf > 0.4:
                valid_kpts.append([x, y])
        
        if len(valid_kpts) < 3: # ì‚¬ëŒì´ ê±°ì˜ ì•ˆ ë³´ì´ë©´ ìŠ¤í‚µ
            return False

        # 2. Segmentation (Teacher A)
        # SAMì€ RGB ì´ë¯¸ì§€ë¥¼ ë°›ìŠµë‹ˆë‹¤.
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        self.sam_predictor.set_image(frame_rgb)
        
        # ViTPoseì˜ ì¢Œí‘œë¥¼ íŒíŠ¸(Point Prompt)ë¡œ ì œê³µ
        input_points = np.array(valid_kpts)
        input_labels = np.ones(len(input_points)) # 1 = Foreground
        
        # Box Prompt ì¶”ê°€ (ëª¸ ì „ì²´ë¥¼ ê°ì‹¸ëŠ” ë°•ìŠ¤) - ì•ˆì •ì„± í–¥ìƒ
        x_min = np.min(input_points[:, 0])
        y_min = np.min(input_points[:, 1])
        x_max = np.max(input_points[:, 0])
        y_max = np.max(input_points[:, 1])
        
        # ë°•ìŠ¤ì— ì—¬ìœ (Padding) ì£¼ê¸°
        h, w = frame.shape[:2]
        pad = 20
        box = np.array([
            max(0, x_min - pad), max(0, y_min - pad),
            min(w, x_max + pad), min(h, y_max + pad)
        ])

        masks, _, _ = self.sam_predictor.predict(
            point_coords=input_points,
            point_labels=input_labels,
            box=box[None, :], # Box íŒíŠ¸ ì¶”ê°€
            multimask_output=False # ëª¨í˜¸í•¨ ì—†ì´ í•˜ë‚˜ë§Œ ì¶œë ¥
        )
        
        final_mask = masks[0] # (H, W) bool array

        # 3. Save Data
        filename = f"{idx:06d}"
        
        # (1) ì›ë³¸ ì´ë¯¸ì§€
        cv2.imwrite(os.path.join(self.out_imgs, f"{filename}.jpg"), frame)
        
        # (2) ë§ˆìŠ¤í¬ (0 or 255)
        mask_uint8 = (final_mask * 255).astype(np.uint8)
        cv2.imwrite(os.path.join(self.out_masks, f"{filename}.png"), mask_uint8)
        
        # (3) ë¼ë²¨ (Keypoints JSON)
        label_data = {
            "keypoints": keypoints.tolist(), # JSON ì§ë ¬í™” ê°€ëŠ¥í•˜ê²Œ ë³€í™˜
            "box": box.tolist()
        }
        with open(os.path.join(self.out_labels, f"{filename}.json"), "w") as f:
            json.dump(label_data, f)
            
        return True

if __name__ == "__main__":
    # ì‚¬ìš©ìê°€ ì…ë ¥í•œ ì„¸ì…˜ ì´ë¦„ (í´ë”ëª…)
    if len(sys.argv) < 2:
        print("ì‚¬ìš©ë²•: python run_labeling.py <SESSION_NAME>")
        print("ì˜ˆ: python run_labeling.py 20231025_143000")
        sys.exit(1)
        
    session_name = sys.argv[1]
    labeler = AutoLabeler(session_name)
    
    # 5í”„ë ˆì„ ê°„ê²© (30fps ì˜ìƒ -> 6fps ë°ì´í„°ì…‹)
    labeler.process(frame_interval=5)