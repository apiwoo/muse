# Project MUSE - run_labeling.py
# The Teacher's Workshop: Multi-Profile Automatic Data Annotation
# (C) 2025 MUSE Corp. All rights reserved.

import os
import sys
import cv2
import numpy as np
import torch
import json
import glob
from tqdm import tqdm

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ ê²½ë¡œ ì¶”ê°€
sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))

# [Teachers]
from ai.tracking.vitpose_trt import VitPoseTrt # Teacher B
try:
    from segment_anything import sam_model_registry, SamPredictor # Teacher A
except ImportError:
    print("âŒ 'segment_anything' ëª¨ë“ˆì´ ì—†ìŠµë‹ˆë‹¤.")
    sys.exit(1)

class AutoLabeler:
    def __init__(self, root_session="personal_data"):
        self.root_dir = os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
        self.root_data_dir = os.path.join(self.root_dir, "recorded_data", root_session)
        
        # [Multi-Profile Search]
        # personal_data/ ì•„ëž˜ì— ìžˆëŠ” ëª¨ë“  í•˜ìœ„ í´ë”(í”„ë¡œíŒŒì¼)ë¥¼ ì°¾ìŠµë‹ˆë‹¤.
        if not os.path.exists(self.root_data_dir):
            print(f"âŒ ë°ì´í„° í´ë”ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.root_data_dir}")
            sys.exit(1)
            
        self.profiles = [
            d for d in os.listdir(self.root_data_dir) 
            if os.path.isdir(os.path.join(self.root_data_dir, d))
        ]
        
        if not self.profiles:
            print(f"âŒ í”„ë¡œíŒŒì¼ í´ë”ê°€ ì—†ìŠµë‹ˆë‹¤. recorder.pyë¥¼ ë¨¼ì € ì‹¤í–‰í•˜ì„¸ìš”.")
            sys.exit(1)
            
        print(f"ðŸ“‚ ë°œê²¬ëœ í”„ë¡œíŒŒì¼: {', '.join(self.profiles)}")

        # Teacher Load
        print("ðŸ‘¨â€ðŸ« [Teacher B] ViTPose(Keypoints) ë¡œë”© ì¤‘...")
        try:
            self.pose_model = VitPoseTrt(engine_path=os.path.join(self.root_dir, "assets/models/tracking/vitpose_huge.engine"))
        except Exception as e:
            print(f"âŒ ViTPose ë¡œë“œ ì‹¤íŒ¨: {e}")
            sys.exit(1)

        print("ðŸ‘©â€ðŸ« [Teacher A] SAM(Segmentation) ë¡œë”© ì¤‘...")
        sam_checkpoint = os.path.join(self.root_dir, "assets/models/segment_anything/sam_vit_h_4b8939.pth")
        if not os.path.exists(sam_checkpoint):
            print(f"âŒ SAM ëª¨ë¸ ì—†ìŒ: {sam_checkpoint}")
            sys.exit(1)
            
        device = "cuda" if torch.cuda.is_available() else "cpu"
        sam = sam_model_registry["vit_h"](checkpoint=sam_checkpoint)
        sam.to(device=device)
        self.sam_predictor = SamPredictor(sam)
        
        print("âœ… ì„ ìƒë‹˜ë“¤ ì¤€ë¹„ ì™„ë£Œ.")

    def process_all_profiles(self, frame_interval=5):
        """ëª¨ë“  í”„ë¡œíŒŒì¼ì„ ìˆœíšŒí•˜ë©° ë¼ë²¨ë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
        for profile in self.profiles:
            print(f"\n==================================================")
            print(f"   Running Labeling for Profile: [{profile}]")
            print(f"==================================================")
            self._process_single_profile(profile, frame_interval)

    def _process_single_profile(self, profile, frame_interval):
        profile_dir = os.path.join(self.root_data_dir, profile)
        video_paths = sorted(glob.glob(os.path.join(profile_dir, "*.mp4")))
        
        if not video_paths:
            print(f"   âš ï¸ ê²½ê³ : '{profile}' í”„ë¡œíŒŒì¼ì— ì˜ìƒì´ ì—†ìŠµë‹ˆë‹¤. ìŠ¤í‚µí•©ë‹ˆë‹¤.")
            return

        # ì¶œë ¥ ë””ë ‰í† ë¦¬ (í”„ë¡œíŒŒì¼ í´ë” ë‚´ë¶€)
        out_imgs = os.path.join(profile_dir, "images")
        out_masks = os.path.join(profile_dir, "masks")
        out_labels = os.path.join(profile_dir, "labels")
        
        for d in [out_imgs, out_masks, out_labels]:
            os.makedirs(d, exist_ok=True)

        # [Append Logic] ê¸°ì¡´ ì¸ë±ìŠ¤ í™•ì¸
        existing_imgs = glob.glob(os.path.join(out_imgs, "*.jpg"))
        max_idx = -1
        if existing_imgs:
            for p in existing_imgs:
                try:
                    name = os.path.splitext(os.path.basename(p))[0]
                    idx = int(name)
                    if idx > max_idx: max_idx = idx
                except: pass
        
        current_idx = max_idx + 1
        print(f"   ðŸš€ ì‹œìž‘ ì¸ë±ìŠ¤: {current_idx}")
        processed_count = 0
        
        for vid_idx, video_path in enumerate(video_paths):
            vid_name = os.path.basename(video_path)
            cap = cv2.VideoCapture(video_path)
            total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
            
            print(f"   [{vid_idx+1}/{len(video_paths)}] {vid_name} ({total_frames} frames)")
            
            frame_idx = 0
            pbar = tqdm(total=total_frames, leave=False)
            
            while cap.isOpened():
                ret, frame = cap.read()
                if not ret: break
                
                if frame_idx % frame_interval == 0:
                    success = self._annotate_frame(frame, current_idx, out_imgs, out_masks, out_labels)
                    if success:
                        current_idx += 1
                        processed_count += 1
                
                frame_idx += 1
                pbar.update(1)
                
            pbar.close()
            cap.release()
            
        print(f"   ðŸŽ‰ [{profile}] ì™„ë£Œ! ì¶”ê°€ëœ ë°ì´í„°: {processed_count}ìž¥")

    def _annotate_frame(self, frame, idx, out_imgs, out_masks, out_labels):
        # 1. Pose
        keypoints = self.pose_model.inference(frame)
        if keypoints is None: return False

        valid_kpts = [kp[:2] for kp in keypoints if kp[2] > 0.4]
        if len(valid_kpts) < 3: return False

        # 2. SAM
        frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        self.sam_predictor.set_image(frame_rgb)
        
        input_points = np.array(valid_kpts)
        input_labels = np.ones(len(input_points))
        
        x_min, y_min = np.min(input_points, axis=0)
        x_max, y_max = np.max(input_points, axis=0)
        h, w = frame.shape[:2]
        pad = 20
        box = np.array([
            max(0, x_min - pad), max(0, y_min - pad),
            min(w, x_max + pad), min(h, y_max + pad)
        ])

        masks, _, _ = self.sam_predictor.predict(
            point_coords=input_points,
            point_labels=input_labels,
            box=box[None, :],
            multimask_output=False
        )
        final_mask = masks[0]

        # 3. Save
        filename = f"{idx:06d}"
        cv2.imwrite(os.path.join(out_imgs, f"{filename}.jpg"), frame)
        cv2.imwrite(os.path.join(out_masks, f"{filename}.png"), (final_mask * 255).astype(np.uint8))
        
        label_data = {"keypoints": keypoints.tolist(), "box": box.tolist()}
        with open(os.path.join(out_labels, f"{filename}.json"), "w") as f:
            json.dump(label_data, f)
            
        return True

if __name__ == "__main__":
    session = sys.argv[1] if len(sys.argv) > 1 else "personal_data"
    labeler = AutoLabeler(session)
    labeler.process_all_profiles(frame_interval=5)